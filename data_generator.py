import pandas as pd
import numpy as np
from faker import Faker
from datetime import datetime
import random
import os
import json


def create_metadata_file(generated_files, num_files, rows_per_file, total_duration, output_dir):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    metadata = {
        'generation_date': datetime.now().isoformat(),
        'total_files': num_files,
        'rows_per_file': rows_per_file,
        'total_rows': num_files * rows_per_file,
        'total_duration_seconds': total_duration,
        'average_speed_rows_per_second': (num_files * rows_per_file) / total_duration if total_duration > 0 else 0,
        'output_directory': os.path.abspath(output_dir),
        'columns': [
            'user_id', 'first_name', 'last_name', 'email',
            'age', 'salary', 'department', 'hire_date',
            'city', 'is_active', 'score', 'last_login',
            'transaction_amount', 'product_category', 'order_date',
            'phone_number', 'postal_code', 'company', 'job_title',
            'credit_score', 'account_balance', 'last_purchase_date'
        ],
        'file_list': [
            {
                'filename': os.path.basename(filepath),
                'path': os.path.abspath(filepath),
                'size_bytes': os.path.getsize(filepath) if os.path.exists(filepath) else 0
            }
            for filepath in generated_files
        ]
    }

    metadata_file = os.path.join(output_dir, '_metadata.json')
    try:
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False, default=str)
        print(f"\nüìÑ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {os.path.abspath(metadata_file)}")
    except Exception as e:
        print(f"\n‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {e}")


def generate_large_csv(filename, rows=10000, batch_size=2000, seed_offset=0, output_dir='data/input'):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è CSV —Ñ–∞–π–ª–∞ —Å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏

    Parameters:
    -----------
    filename : str
        –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    rows : int
        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ —Ñ–∞–π–ª–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10,000)
    batch_size : int
        –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2,000)
    seed_offset : int
        –°–º–µ—â–µ–Ω–∏–µ –¥–ª—è seed, —á—Ç–æ–±—ã –¥–∞–Ω–Ω—ã–µ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö –±—ã–ª–∏ —Ä–∞–∑–Ω—ã–º–∏
    output_dir : str
        –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'data/input')
    """
    fake = Faker('ru_RU')  # –†—É—Å—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ

    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ seed –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
    Faker.seed(42 + seed_offset)
    np.random.seed(42 + seed_offset)
    random.seed(42 + seed_offset)

    columns = [
        'user_id', 'first_name', 'last_name', 'email',
        'age', 'salary', 'department', 'hire_date',
        'city', 'is_active', 'score', 'last_login',
        'transaction_amount', 'product_category', 'order_date',
        'phone_number', 'postal_code', 'company', 'job_title',
        'credit_score', 'account_balance', 'last_purchase_date'
    ]

    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ—Ç
    os.makedirs(output_dir, exist_ok=True)
    filepath = os.path.join(output_dir, filename)

    print(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∞–π–ª–∞ '{filename}' —Å {rows:,} —Å—Ç—Ä–æ–∫–∞–º–∏...")
    start_time = datetime.now()

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –±–∞—Ç—á–∞–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    for batch_num in range(0, rows, batch_size):
        batch_rows = min(batch_size, rows - batch_num)
        batch_data = []

        for i in range(batch_rows):
            user_id = batch_num + i + 1
            first_name = fake.first_name()
            last_name = fake.last_name()
            email = fake.email()
            age = np.random.randint(18, 75)
            salary = round(np.random.normal(60000, 20000), 2)
            department = np.random.choice(['IT', 'Sales', 'HR', 'Marketing', 'Finance', 'Operations', 'Support', 'R&D'])
            hire_date = fake.date_between(start_date='-10y', end_date='today')
            city = fake.city()
            is_active = np.random.choice([True, False], p=[0.85, 0.15])
            score = round(np.random.uniform(0, 100), 2)
            last_login = fake.date_time_between(start_date='-60d', end_date='now')
            transaction_amount = round(np.random.exponential(150), 2)
            product_category = np.random.choice(
                ['Electronics', 'Clothing', 'Food', 'Books', 'Home', 'Automotive', 'Health', 'Sports'])
            order_date = fake.date_between(start_date='-180d', end_date='today')
            phone_number = fake.phone_number()
            postal_code = fake.postcode()
            company = fake.company()
            job_title = fake.job()
            credit_score = np.random.randint(300, 850)
            account_balance = round(np.random.uniform(-5000, 50000), 2)
            last_purchase_date = fake.date_between(start_date='-365d',
                                                   end_date='today') if np.random.random() > 0.3 else None

            batch_data.append([
                user_id, first_name, last_name, email, age, salary,
                department, hire_date, city, is_active, score,
                last_login, transaction_amount, product_category, order_date,
                phone_number, postal_code, company, job_title,
                credit_score, account_balance, last_purchase_date
            ])

        # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –±–∞—Ç—á–∞
        df_batch = pd.DataFrame(batch_data, columns=columns)

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ —Ñ–∞–π–ª (–¥–æ–±–∞–≤–ª—è–µ–º –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–≤—ã–π –±–∞—Ç—á)
        if batch_num == 0:
            df_batch.to_csv(filepath, index=False)
        else:
            df_batch.to_csv(filepath, mode='a', header=False, index=False)

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        completed = batch_num + batch_rows
        progress = (completed / rows) * 100
        print(f"  –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}% ({completed:,}/{rows:,} —Å—Ç—Ä–æ–∫)")

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    print(f"‚úì –§–∞–π–ª '{filename}' —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω!")
    print(f"  –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {duration:.2f} —Å–µ–∫—É–Ω–¥")
    print(f"  –°–∫–æ—Ä–æ—Å—Ç—å: {rows / duration:.0f} —Å—Ç—Ä–æ–∫/—Å–µ–∫")
    print(f"  –ü–æ–ª–Ω—ã–π –ø—É—Ç—å: {os.path.abspath(filepath)}\n")
    return filepath


def generate_filenames(num_files=15):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –∏–º–µ–Ω –¥–ª—è —Ñ–∞–π–ª–æ–≤ —Å –Ω–æ–º–µ—Ä–∞–º–∏ –ø–∞—á–µ–∫

    Parameters:
    -----------
    num_files : int
        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    """
    current_date = datetime.now().strftime("%Y%m%d")
    current_time = datetime.now().strftime("%H%M%S")  # –î–æ–±–∞–≤–ª–µ–Ω–æ: –¢–û–õ–¨–ö–û –≤—Ä–µ–º—è —á–∞—Å-–º–∏–Ω—É—Ç–∞-—Å–µ–∫—É–Ω–¥–∞

    filenames = []
    for i in range(1, num_files + 1):
        # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: data_batch_001_20241218_143025.csv
        filename = f"data_batch_{i:03d}_{current_date}_{current_time}.csv"
        filenames.append(filename)

    return filenames


# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è 15 —Ñ–∞–π–ª–æ–≤ –ø–æ 10,000 —Å—Ç—Ä–æ–∫
if __name__ == "__main__":
    files_to_generate = 15
    rows_per_file = 10000
    output_directory = 'data/input'  # –ù–æ–≤–∞—è –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

    print("=" * 70)
    print("üöÄ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ë–û–õ–¨–®–ò–• –§–ê–ô–õ–û–í –î–õ–Ø –î–ê–¢–ê-–ò–ù–ñ–ï–ù–ï–†–ê")
    print("=" * 70)
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤: {files_to_generate}")
    print(f"–°—Ç—Ä–æ–∫ –≤ –∫–∞–∂–¥–æ–º —Ñ–∞–π–ª–µ: {rows_per_file:,}")
    print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {files_to_generate * rows_per_file:,}")
    print(f"–û—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä: ~{files_to_generate * rows_per_file * 300 / (1024 * 1024):.1f} MB")
    print(f"–ü–∞–ø–∫–∞ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è: {os.path.abspath(output_directory)}")
    print("=" * 70)

    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs(output_directory, exist_ok=True)

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤
    filenames = generate_filenames(files_to_generate)

    total_start_time = datetime.now()
    generated_files = []

    for i, filename in enumerate(filenames, 1):
        print(f"\n[{i}/{files_to_generate}] –ù–∞—á–∞–ª–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {filename}")

        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º seed_offset –¥–ª—è –±–æ–ª–µ–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏
        seed_offset = i * 10000  # –ë–æ–ª—å—à–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ä–∞–∑–ª–∏—á–∏–π

        filepath = generate_large_csv(
            filename=filename,
            rows=rows_per_file,
            batch_size=2000,  # –£–≤–µ–ª–∏—á–∏–ª–∏ –±–∞—Ç—á –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            seed_offset=seed_offset,
            output_dir=output_directory  # –ü–µ—Ä–µ–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        )

        generated_files.append(filepath)

        # –û—Ü–µ–Ω–∫–∞ –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –≤—Ä–µ–º–µ–Ω–∏
        if i > 1:
            elapsed = (datetime.now() - total_start_time).total_seconds()
            avg_time_per_file = elapsed / i
            remaining_files = files_to_generate - i
            estimated_remaining = avg_time_per_file * remaining_files

            print(f"   –û—Å—Ç–∞–ª–æ—Å—å: {remaining_files} —Ñ–∞–π–ª–æ–≤ (~{estimated_remaining:.0f} —Å–µ–∫)")

    total_end_time = datetime.now()
    total_duration = (total_end_time - total_start_time).total_seconds()

    print("\n" + "=" * 70)
    print("‚úÖ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
    print("=" * 70)

    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    total_size = 0
    for filepath in generated_files:
        if os.path.exists(filepath):
            total_size += os.path.getsize(filepath)

    total_rows = files_to_generate * rows_per_file

    print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –°–æ–∑–¥–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(generated_files)}")
    print(f"   –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_rows:,}")
    print(f"   –û–±—â–∏–π –æ–±—ä–µ–º: {total_size / (1024 * 1024):.2f} MB")
    print(f"   –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {total_size / len(generated_files) / (1024 * 1024):.2f} MB")
    print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_duration:.2f} —Å–µ–∫—É–Ω–¥")
    print(f"   –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {total_rows / total_duration:.0f} —Å—Ç—Ä–æ–∫/—Å–µ–∫")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ñ–∞–π–ª: {total_duration / len(generated_files):.2f} —Å–µ–∫")

    print(f"\nüìÅ –ü–ê–ü–ö–ê –° –î–ê–ù–ù–´–ú–ò: {os.path.abspath(output_directory)}")

    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    create_metadata_file(generated_files, files_to_generate, rows_per_file, total_duration, output_directory)

    print(f"\nüìã –°–ü–ò–°–û–ö –°–û–ó–î–ê–ù–ù–´–• –§–ê–ô–õ–û–í:")
    if os.path.exists(output_directory):
        files = sorted([f for f in os.listdir(output_directory) if f.endswith('.csv')])
        for idx, file in enumerate(files, 1):
            filepath = os.path.join(output_directory, file)
            size = os.path.getsize(filepath)
            print(f"   {idx:2d}. {file} ({size:,} –±–∞–π—Ç)")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç—å –∫ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    print(f"\nüìç –¢–ï–ö–£–©–ê–Ø –î–ò–†–ï–ö–¢–û–†–ò–Ø: {os.getcwd()}")